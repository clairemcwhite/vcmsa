#!/usr/bin/env python3

# To generate embeddings
import torch
from transformers import AutoTokenizer, AutoModel

# To calculate similarity
import faiss
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import normalize



from vcmsa.src.vcmsa_embed import parse_fasta_for_embed, get_embeddings
from vcmsa.src.vcmsa_utils import *
 
#from vcmsa.utils import build_index_flat
#from vcmsa.embed import parse_fasta_for_embed, get_embeddings 
#from vcmsa.seqsim import get_seqsims

# Printing for debugging with icecream
try:
    from icecream import ic
    ic.configureOutput(includeContext=True, outputFunction=print) # Prints line number and function
except ImportError:  # Graceful fallback if IceCream isn't installed.
    ic = lambda *a: None if not a else (a[0] if len(a) == 1 else a)  # noqa

# This is combat with patsy requirement removed
from transformer_infrastructure.combat2 import combat

# For networks
import igraph

# For copying graph objects
import copy

# For parsing fasta
from Bio import SeqIO

# To layout alignment
from Bio.Align import MultipleSeqAlignment
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord

# For plotting PCA
import matplotlib.pyplot as plt
import random



# General purpose
import pickle
import argparse
import numpy as np
from pandas.core.common import flatten 
import pandas as pd 
from collections import Counter
import logging


# File IO
import os
import shutil
import sys


def get_align_args():

    parser = argparse.ArgumentParser()
    parser.add_argument("-i", "--in", dest = "fasta_path", type = str, required = True,
                        help="Path to fasta")
    
    parser.add_argument("-e", "--emb", dest = "embedding_path", type = str, required = False,
                        help="Path to embeddings")

    parser.add_argument("-o", "--outfile", dest = "out_path", type = str, required = True,
                        help="Path to outfile")

    parser.add_argument("-bc", "--batch_correct", dest = "batch_correct", action = "store_true", 
                        help="If added, do batch correction on sequences")
    parser.add_argument("-sl", "--seqlimit", dest = "seqlimit", type = int, required = False,
                        help="Limit to n sequences. For testing")

    parser.add_argument("-ex", "--exclude", dest = "exclude", action = "store_true",
                        help="Exclude outlier sequences from initial alignment process")


    parser.add_argument("-fx", "--fully_exclude", dest = "fully_exclude", action = "store_true",
                        help="Additionally exclude outlier sequences from final alignment")

    parser.add_argument("-l", "--layers", dest = "layers", nargs="+", type = int, default = [-16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1],
                        help="Which layers (of 30 in protbert) to select default = -16 -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 -5 -4 -3 -2 -1'")
    parser.add_argument("-hd", "--heads", dest = "heads", type = str,
                        help="File will one head identifier per line, format layer1_head3")

    parser.add_argument("-st", "--seqsimthresh", dest = "seqsimthresh",  type = float, required = False, default = 0.7,
                        help="Similarity threshold for clustering sequences, default = 0.7")


    parser.add_argument("-m", "--model", dest = "model_name",  type = str, required = True,
                        help="Model name or path to local model")

    parser.add_argument("-pca", "--pca_plot", dest = "pca_plot",  action = "store_true", required = False, 
                        help="If flagged, output 2D pca plot of amino acid clusters")

    parser.add_argument("-p", "--padding", dest = "padding",  type = int, required = False, default = 0, 
                        help="Number of characters of X to add to start and end of sequence (can be important for fragment sequences), default: 0")
    parser.add_argument("-l2", "--headnorm", dest = "headnorm",  action = "store_true", required = False, 
                        help="Take L2 normalization of each head")
    args = parser.parse_args()

    return(args)




 
def get_seq_groups(seqs, seq_names, embedding_dict, logging, exclude, do_clustering, outfile_name, record_dir, seqsim_thresh = 0.75):

    aa_embeddings = embedding_dict['aa_embeddings'] # this is numseqs x padded_seqlen x embedding_dim
    padded_seqlen = aa_embeddings.shape[1]


    logging.info("Flattening hidden states list")
    hidden_states = np.array(reshape_flat(aa_embeddings)) # This is now numseqs*padded_seqlen x embedding_dim
    seqnums = list(range(0, len(seqs)))
    seqs_aas, seq_to_length = get_seqs_aas(seqs, seqnums)
    index_to_aa, hidden_states, seqnum_to_index, batch_list = remove_maxlen_padding(hidden_states, seqs_aas, padded_seqlen)
    pca_plot = True 
    if pca_plot:
       png_hidden_out = "{}/{}.initial.prebatch.png".format(record_dir, outfile_name)
       do_pca_plot(hidden_states, index_to_aa, png_hidden_out, seq_to_length = seq_to_length)
    
    # Don't batch correct prior to sequence clustering
    #batch_correct = False
    #if batch_correct:
    #  
    #    #ic( list(range(len(seqs_aas)))) 
    #    hidden_states = do_batch_correct(hidden_states, list(range(len(seqs_aas))), batch_list)
    #       
    #    if pca_plot:
    #        png_hidden_out = "{}/{}.initial.postbatch.png".format(record_dir, outfile_name)
    #        do_pca_plot(hidden_states, index_to_aa, png_hidden_out, seq_to_length = seq_to_length)
    

    sequence_embedding_list = []
    aa_embedding_list = []
    for i in range(len(seqs_aas)):
          seq_indices = seqnum_to_index[i]
          seq_i_aa_embeddings = np.take(hidden_states, seq_indices, axis = 0)
          aa_embedding_list.append(seq_i_aa_embeddings)
          #ic(seq_i_aa_embeddings.shape)
          seq_i_seq_embedding = np.mean(seq_i_aa_embeddings, axis = 0)
          #ic(seq_i_seq_embedding.shape)
          sequence_embedding_list.append(seq_i_seq_embedding)

    sequence_array = np.array(sequence_embedding_list)
    numseqs = len(seqs)
    G, s_index = get_seqsims(sequence_array, k = numseqs)
    to_exclude = []

   
    group_hstates_list = []
    cluster_seqnums_list = []
    cluster_names_list = []
    cluster_seqs_list = []
   

    logging.info("Removing spaces from sequences")
    if do_clustering == True:
        #ic("fastgreedy")
        #ic(G)
    
      repeat = True
      while repeat == True:

        group_hstates_list = []
        cluster_seqnums_list = []
        cluster_names_list = []
        cluster_seqs_list = []
 
        prev_to_exclude = to_exclude
        

    
        ic("GG", G.vs()['name'])
        ic("GG", G.es()['weight'])
        edgelist = []
        weightlist = []
        for edge in G.es():
             if G.vs[edge.target]["name"] not in to_exclude:
                  if G.vs[edge.source]["name"] not in to_exclude:
                     if edge['weight'] >= seqsim_thresh:
                         #if edge.source != edge.target:
                             source = G.vs[edge.source]["name"]
                             target = G.vs[edge.target]["name"]
                        
 
 
                             length_diff_correction = 1 - abs(0.5 - len(seqs[source]) / (len(seqs[source]) + len(seqs[target])))
                             corrected_weight = edge['weight'] * length_diff_correction
                             print("seqsim: ", source,target, edge['weight'], "l1, l2", len(seqs[source]), len(seqs[target]), corrected_weight)
                             if corrected_weight >= seqsim_thresh:
                             
                                 edgelist.append([ source, target ])
                                 weightlist.append(corrected_weight)
        # Rebuild G
        G = igraph.Graph.TupleList(edges=edgelist, directed=False)
        G.es['weight'] = weightlist

        G = G.simplify(combine_edges=max)
        ic("G", G)

        seq_clusters = G.community_walktrap(steps = 3, weights = 'weight').as_clustering() 
        if len(seq_clusters.subgraphs()) == len(G.vs()):
             seq_clusters = G.clusters(mode = "weak") # walktrap can cluster nodes individually. See UBQ

        ic("After walktrap", seq_clusters)
        for seq_cluster_G in seq_clusters.subgraphs():
        
                # Do exclusion within clusters
                ic("seq_clusters", seq_cluster_G)
                if exclude == True:
    
                    clust_names = seq_cluster_G.vs()["name"]
                    ic("clust_names", clust_names)
                    cluster_to_exclude = candidate_to_remove(seq_cluster_G, clust_names, z = -5)
                    ic(cluster_to_exclude)
                       
                    to_delete_ids_sub_G = [v.index for v in seq_cluster_G.vs if v['name'] in cluster_to_exclude]
                    seq_cluster_G.delete_vertices(to_delete_ids_sub_G) 
    
    
                    ic("to_exclude_pre", to_exclude)
                    to_exclude = to_exclude + cluster_to_exclude
                    to_exclude = list(set(to_exclude))
                    ic("to_exclude_post", to_exclude)
                    if to_exclude:       
                        logging.info("Excluding following sequences: {}".format(",".join([str(x) for x in to_exclude])))
                        ic("Excluding following sequences: {}".format(",".join([str(x) for x in to_exclude])))
    
                hstates = []
                seq_cluster = seq_cluster_G.vs()['name']
                seq_cluster.sort()
                ic(seq_cluster)
                cluster_seqnums_list.append(seq_cluster)
        
                filter_indices = seq_cluster
                group_hstates = np.take(embedding_dict['aa_embeddings'], filter_indices, axis = 0)
                group_hstates_list.append(group_hstates)
        
                cluster_names = [seq_names[i] for i in filter_indices]
                cluster_names_list.append(cluster_names)
           
                cluster_seq = [seqs[i] for i in filter_indices]
                cluster_seqs_list.append(cluster_seq)
                to_exclude = list(set(to_exclude))
        ic("eq check", to_exclude, prev_to_exclude)
        if set(to_exclude) == set(prev_to_exclude):
           repeat = False
        else:
               cluster_seqs_list = [] 
               cluster_seqnums_list = []
               group_hstates_list = []
               cluster_names_list= []
    else:
         if exclude == True:
            clust_names = G.vs()["name"] 
            to_exclude = candidate_to_remove(G, clust_names, z = -3)
            ic('name', to_exclude)
            to_delete_ids = [v.index for v in G.vs if v['name'] in to_exclude]
            #ic('vertix_id', to_delete_ids)
            G.delete_vertices(to_delete_ids) 
    
            logging.info("Excluding following sequences: {}".format(",".join([str(x) for x in to_exclude])))
    
         else:
           logging.info("Not removing outlier sequences")
           to_exclude = []
 
 
        # #ic([v['name'] for v in G.vs])
         cluster_seqnums_list =  [v['name'] for v in G.vs]
         ic(cluster_seqnums_list, to_exclude)
         cluster_seqnums_list = list(set(cluster_seqnums_list))
         cluster_seqnums_list.sort()
         # Make sure this is removing to_exclude corectly
         cluster_seqs_list = [[seqs[i] for i in cluster_seqnums_list]]
         cluster_names_list = [[seq_names[i] for i in cluster_seqnums_list]]
         group_hstates_list = [np.take(embedding_dict['aa_embeddings'], cluster_seqnums_list, axis = 0)]
         cluster_seqnums_list = [cluster_seqnums_list] 
         to_exclude = list(set(to_exclude))

    ic("seqnum clusters", cluster_seqnums_list)
    ic(cluster_names_list)
    # Anything lost in the processes

    ic("cluster_seqnums_list", cluster_seqnums_list) 
    dropped_seqs = [ x for x in list(range(0,len(seqs))) if x not in flatten(cluster_seqnums_list)]
    ic("Dropped seqs:", dropped_seqs)
    to_exclude = list(set(to_exclude + dropped_seqs))    
    ic("final to exclude", to_exclude)

    return(cluster_seqnums_list, cluster_seqs_list,  cluster_names_list, group_hstates_list, to_exclude)


def do_msa(seqs, seq_names, seqnums, hstates_list, logging, minscore1 = 0.5, alignment_group = 0, args = None):
    """
    Required args: record_dir, outfile_name, do_pca, headnorm
    Control for running whole alignment process
    """
    pca_plot = args.pca_plot # default True
    headnorm = args.headnorm # default False
    record_dir = args.record_dir
    outfile_name = args.outfile_name
    batch_correct = args.batch_correct
    seqlens = [len(x) for x in seqs]
    ic("seqs", seqs, seqlens)

     
    numseqs = len(seqs)
    padded_seqlen = hstates_list.shape[1]
    embedding_length = hstates_list.shape[2]
    ic("numseqs", numseqs)
    ic("padded_seqlen", padded_seqlen)
    ic(hstates_list.shape)


    logging.info("Flattening hidden states list")
    hidden_states = np.array(reshape_flat(hstates_list))  
    logging.info("embedding_shape: {}".format(hidden_states.shape))


    
    logging.info("Convert index position to amino acid position")

    seqs_aas, seq_to_length = get_seqs_aas(seqs, seqnums)
    index_to_aa, hidden_states, seqnum_to_index, batch_list = remove_maxlen_padding(hidden_states, seqs_aas, padded_seqlen)
    if pca_plot:
        png_hidden_out = "{}/{}.alignment_group{}.prebatch.png".format(record_dir, outfile_name, alignment_group)
        do_pca_plot(hidden_states, index_to_aa, png_hidden_out, seq_to_length = seq_to_length)


    if batch_correct:
        ic( list(range(len(seqs_aas)))) 
        hidden_states = do_batch_correct(hidden_states, list(range(len(seqs_aas))), batch_list)
       
        if pca_plot:
            png_hidden_out = "{}/{}.alignment_group{}.postbatch.png".format(record_dir, outfile_name, alignment_group)
            do_pca_plot(hidden_states, index_to_aa, png_hidden_out, seq_to_length = seq_to_length)

     
    faiss.normalize_L2(hidden_states)
    index= build_index_flat(hidden_states, scoretype = "cosinesim", normalize_l2 = False) # Already normalized
    logging.info("Index built") 

    if pca_plot:
        png_hidden_out = "{}/{}.alignment_group{}.postnorm.png".format(record_dir, outfile_name, alignment_group)
        do_pca_plot(hidden_states, index_to_aa, png_hidden_out, seq_to_length = seq_to_length)


    # Get KNN of each amino acid
    D1, I1 =  index.search(hidden_states, k = numseqs*20) 

    
 
    logging.info("KNN done")
    
    I2 = split_distances_to_sequence(D1, I1, index_to_aa, numseqs, seqlens) 
    # I2 is a dictionary of dictionaries of each aa1: {aa1:1.0, aa2:0.8}

    logging.info("Split results into proteins done")

    logging.info("get best hitlist")
    minscore1 = minscore1
 
    hitlist_all = get_besthits(I2, minscore = 0) # High threshold for first clustering 
    
    for x in hitlist_all:
       print("hitlist_all:", x)
    logging.info("got best hitlist")

  
    logging.info("get reciprocal best hits")

    rbh_list = get_rbhs(hitlist_all)  
    logging.info("got reciprocal best hits")
   

    for x in rbh_list:
      print("rbh", x) 
   
    outnet = "{}/{}.testnet_initial_clustering{}.csv".format(record_dir, outfile_name, alignment_group)
    with open(outnet, "w") as outfile:
          outfile.write("aa1,aa2,score\n")
          for x in rbh_list:
             outstring = "{},{},{}\n".format(x[0], x[1], x[2])        
             outfile.write(outstring)


    ic("Start betweenness calculation to filter cluster-connecting amino acids. Also first round clustering")

    G = graph_from_rbh(rbh_list, directed = False)
 

    clusters_list = []
    if len(seqs) > 2:
        minclustsize = int(len(seqs)/2) + 1
        if len(clusters_list) == 0:
            clusters_list, all_alternates_dict = first_clustering(G, betweenness_cutoff = 0.1, ignore_betweenness = False, apply_walktrap = True)

    else:
        minclustsize = 2
        if len(clusters_list) == 0:
            clusters_list, all_alternates_dict = first_clustering(G, betweenness_cutoff = 1, ignore_betweenness = True, apply_walktrap = True)

       
    clusters_list = [x for x in clusters_list if len(x) > 1]
    for x in clusters_list:
        ic("First clusters", x)

    # Why is dedup_clusters only used once? 
    new_clusters_list = dedup_clusters(clusters_list, G, minclustsize)
    for x in new_clusters_list:
      ic("Deduplicated first clusters", x)


    clusters_filt = []
    too_small = []
    for clust in new_clusters_list:
          if len(clust) >= minclustsize:
                clusters_filt.append(clust)
          else:
             # This is ever happening?
             if len(clust) > 2:
                too_small.append(clust)
    for x in clusters_filt:
          ic("First clusters with small removed", x)
    for x in too_small:
          ic("collected as too_small", x)
    ic("Getting DAG of cluster orders, removing feedback loops")
    cluster_order, clustid_to_clust, pos_to_clustid, alignment = organize_clusters(clusters_filt, seqs_aas, gapfilling_attempt = 0, minclustsize = minclustsize, all_alternates_dict = all_alternates_dict, seqnames = seq_names, args = args)

    clusters_filt = list(clustid_to_clust.values())  

    for x in clusters_filt:
          ic("First clusters after feedback loop removal", x)
    alignment = make_alignment(cluster_order, seqnums, clustid_to_clust, seq_names)

    if len(seqnums) > 2:
       if len(seqnums) < 5:
            minclustsize = len(seqnums) - 1
       else:
            minclustsize = 4
    else:
       minclustsize = 2

    ignore_betweenness = False
    minscore = 0.5
    betweenness_cutoff = 0.30
    history_unassigned = {'onebefore':[], 'twobefore':[], 'threebefore':[]}
    print(alignment)
    most_complete_alignment = alignment
    too_small = [] 
    rbh_dict = {}
    match_dict = {}


    ############## CONTROL LOOP ###################
    for gapfilling_attempt in range(0, 200):
        gapfilling_attempt = gapfilling_attempt + 1
        print("Align this is gapfilling attempt ", gapfilling_attempt)
        logging.info("gapfilling_attempt {}".format(gapfilling_attempt))
        if gapfilling_attempt > 6 and minclustsize > 2 and gapfilling_attempt % 2 == 1:
                minclustsize = minclustsize - 1
        ic("This is the minclustsize", minclustsize)
         
        unassigned = get_unassigned_aas(seqs_aas, pos_to_clustid, too_small)
        for x in unassigned:
           ic("unassign", x)

        if len(unassigned) == 0:
            ic("Alignment complete after gapfilling attempt {}".format(gapfilling_attempt - 1))
     
            alignment = make_alignment(cluster_order, seqnums, clustid_to_clust, seq_names)
            return(alignment, index, hidden_states, index_to_aa)

        if ( unassigned == history_unassigned['threebefore'] or  unassigned == history_unassigned['twobefore'] ) and gapfilling_attempt > 10:
            if minscore > 0.1:
                minscore = 0.1
                ic("reducing minscore to {} at gapfilling attempt {}".format(minscore, gapfilling_attempt))
            ################ Final stage #############3
            else: 
                alignment = make_alignment(cluster_order, seqnums, clustid_to_clust, seq_names)
                ic("current {}, previous record {}".format(alignment.numassigned,most_complete_alignment.numassigned))
                if alignment.numassigned < most_complete_alignment.numassigned:
                    ic("Replacing current alignment with previous more complete alignment")
                    alignment = most_complete_alignment
                else:
                    ic("Currently alignment is the most complete so far")
                ic("Align by placing remaining amino acids")
                cluster_order, clustid_to_clust, pos_to_clustid, alignment = fill_in_hopeless(unassigned, seqs_aas, cluster_order, clustid_to_clust, pos_to_clustid, index, hidden_states, gapfilling_attempt, args = args)
                unassigned = get_unassigned_aas(seqs_aas, pos_to_clustid)
                ic('This unassigned should be empty', unassigned)
                alignment = make_alignment(cluster_order, seqnums, clustid_to_clust, seq_names)

                return(alignment,   index, hidden_states, index_to_aa)
 
        history_unassigned['threebefore'] = history_unassigned['twobefore']
        history_unassigned['twobefore'] = history_unassigned['onebefore']
        history_unassigned['onebefore'] = unassigned
        
        apply_walktrap = False

        # Do one or two rounds of clustering between guideposts
        if gapfilling_attempt in list(range(1, 100,2)):#  or gapfilling_attempt in [1, 2, 3, 4]:

            ic("Align by clustering within guideposts")
            # Don't allow modification of previous guideposts
            if gapfilling_attempt > 4:
                 apply_walktrap = True

            if gapfilling_attempt > 3:
              if gapfilling_attempt < 15: 
                # This removes 'stranded' amino acids, where neither the previous or next amino acid are placed adjacent. 
                # If a good cluster, will be added right back
               cluster_order, clustid_to_clust = address_stranded(alignment)
               alignment = make_alignment(cluster_order, seqnums, clustid_to_clust, seq_names) 
               clusterlist = list(clustid_to_clust.values())
               new_clusterlist = []
               pos_to_clustid, clustid_to_clust = get_cluster_dict(clusterlist)
               unassigned = get_unassigned_aas(seqs_aas, pos_to_clustid)

            cluster_order, clustid_to_clust, pos_to_clustid, alignment, too_small, rbh_dict, all_new_rbh = fill_in_unassigned_w_clustering(unassigned, seqs_aas, cluster_order, clustid_to_clust, pos_to_clustid, I2,  gapfilling_attempt, minscore = minscore ,minclustsize = minclustsize, ignore_betweenness = ignore_betweenness, betweenness_cutoff = betweenness_cutoff, apply_walktrap = apply_walktrap, rbh_dict = rbh_dict, seqnames = seq_names, args = args)
 
            outnet = "{}/{}.testnet_clustering_group_{}_gapfilling_{:04}.csv".format(record_dir, outfile_name, alignment_group, gapfilling_attempt)
            with open(outnet, "w") as outfile:
                  outfile.write("aa1,aa2,score\n")
                  # If do reverse first, don't have to do second resort
                  for x in all_new_rbh:
                     outstring = "{},{},{}\n".format(x[0], x[1], x[2])        
                     outfile.write(outstring)


 
            for x in too_small:
               ic("collected as too_small after clustering", x)        

            for key,value in clustid_to_clust.items():
                 ic(key, value)

        else:
            
            ic("Align by best match (looser)")
            logging.info("Add aa's to existing clusters")
            if gapfilling_attempt > 3:
              if gapfilling_attempt < 15: 
                # This removes 'stranded' amino acids. 
               alignment = make_alignment(cluster_order, seqnums, clustid_to_clust, seq_names) 
               clusterlist = list(clustid_to_clust.values())
               new_clusterlist = []
               pos_to_clustid, clustid_to_clust = get_cluster_dict(clusterlist)
               unassigned = get_unassigned_aas(seqs_aas, pos_to_clustid)
            cluster_order, clustid_to_clust, pos_to_clustid, alignment, match_dict = fill_in_unassigned_w_search(unassigned, seqs_aas, cluster_order, clustid_to_clust, pos_to_clustid, index, hidden_states,  index_to_aa, gapfilling_attempt, minclustsize = minclustsize, remove_both = True, match_dict= match_dict, seqnames = seq_names, args = args)
            too_small = []
            #cluster_order_merge, clustid_to_clust_merge, pos_to_clustid_merge, alignment)


        # THINGS TO DO AT THE END OF GAPFILLING CYCLE
        if alignment.numassigned > most_complete_alignment.numassigned:
            ic("This is the most complete alignment so far")
            most_complete_alignment = alignment
   
        outaln = "{}/{}.alignment_clustering_group_{}_gapfilling_{:04}.aln".format(record_dir, outfile_name, alignment_group, gapfilling_attempt)
        with open(outaln, "w") as outfile:
            outfile.write(alignment.format_aln("clustal"))

           
            #ic(alignment_ic(alignment, seq_names)[0])
    return( alignment,  index, hidden_states,  index_to_aa)   



if __name__ == '__main__':

    args = get_align_args()

    print("args", args)
    fasta_path = args.fasta_path
    embedding_path = args.embedding_path
    outfile = args.out_path
    exclude = args.exclude
    fully_exclude = args.fully_exclude
    layers = args.layers
    heads = args.heads
    model_name = args.model_name
    pca_plot = args.pca_plot
    seqlimit = args.seqlimit
    headnorm = args.headnorm
    padding =args.padding
    seqsim_thresh  = args.seqsimthresh 
    do_clustering = True

    # get place to store output file
    outfile_path =  os.path.dirname(outfile)     
    outfile_name =   os.path.splitext(os.path.basename(outfile))[0] # get outfile without extension
    if outfile_path:
       record_dir = "{}/alignment_files_{}".format(outfile_path,outfile_name)
    else:
       record_dir = "alignment_files_{}".format(outfile_name)
    if os.path.exists(record_dir):
        shutil.rmtree(record_dir, ignore_errors=True)
    
    os.mkdir(record_dir)

    args.outfile_name  = outfile_name
    args.record_dir = record_dir

    print("Params", args)
    logname = "{}/{}.align.log".format(record_dir, outfile_name)
    log_format = "%(asctime)s::%(levelname)s::"\
             "%(filename)s::%(lineno)d::%(message)s"
    logging.basicConfig(filename=logname, level='DEBUG', format=log_format)



    if heads is not None:
       with open(heads, "r") as f:
         headnames = f.readlines()
         ic(headnames)
         headnames = [x.replace("\n", "") for x in headnames]

         ic(headnames)
    else:
       headnames = None
    logging.info("Check for torch")
    logging.info(torch.cuda.is_available())

    minscore1 = 0.5

    logging.info("model: {}".format(model_name))
    logging.info("fasta: {}".format(fasta_path))
    logging.info("padding: {}".format(padding))
    logging.info("first score thresholds: {}".format(minscore1))
    #seq_names, seqs, seqs_spaced = parse_fasta_for_embed(fasta_path, padding = padding)
   
    seq_names, seqs, seqs_spaced = parse_fasta_for_embed(fasta_path, padding = padding)

    if seqlimit:
       seq_names = seq_names[0:seqlimit]
       seqs = seqs[0:seqlimit]
       seqs_spaced = seqs_spaced[0:seqlimit]

 
    ic("Sequences", seqs)    

    # The first step is to get embeddings
    if embedding_path:
       with open(embedding_path, "rb") as f:
             embedding_dict = pickle.load(f)

    else:
        seqlens = [len(x) for x in seqs]
        embedding_dict = get_embeddings(seqs_spaced,
                                    model_name,
                                    seqlens = seqlens,
                                    get_sequence_embeddings = True,
                                    get_aa_embeddings = True,
                                    layers = layers,  
                                    padding = padding,
                                    heads = headnames)


    # The sequences are divided into groups
    ic(seqsim_thresh)
    cluster_seqnums_list, cluster_seqs_list,  cluster_names_list, cluster_hstates_list, to_exclude = get_seq_groups(seqs ,seq_names, embedding_dict, logging, exclude, do_clustering, outfile_name, record_dir, seqsim_thresh = seqsim_thresh)

    aln_fasta_list = []
    excluded_records = []
    for excluded_seqnum in to_exclude:
         
         excluded_record = SeqRecord(Seq(seqs[excluded_seqnum]), id=seq_names[excluded_seqnum], description = '')
         excluded_records.append(excluded_record)
         # Option to keep poor matches out
         if fully_exclude != True:
            aln_fasta_list.append([">{}\n{}\n".format(seq_names[excluded_seqnum], seqs[excluded_seqnum])])
   

    with open("{}/{}.excluded.fasta".format(record_dir, outfile_name), "w") as output_handle:
        SeqIO.write(excluded_records, output_handle, "fasta")

    alignments = []
    hidden_states_list = []
    index_to_aas_list = []

    # For each sequence group, do a sub alignment
    for i in range(len(cluster_names_list)):
        group_seqs = cluster_seqs_list[i]

             

        group_seqnums = cluster_seqnums_list[i]
        group_names = cluster_names_list[i]
        group_embeddings = cluster_hstates_list[i] 
        ic("group seqnames", group_names, group_seqnums)

        group_seqs_out = "{}/{}.alignment_group{}.fasta".format(record_dir, outfile_name, i)
        group_records = []

        for j in range(len(group_seqs)):
             group_records.append(SeqRecord(Seq(group_seqs[j]), id=group_names[j], description = ''))
 
        with open(group_seqs_out, "w") as output_handle:
            SeqIO.write(group_records, output_handle, "fasta")

        if len(group_names) ==  1:
             aln_fasta_list.append([">{}\n{}\n".format(group_names[0], group_seqs[0])])


        else:
            # Main function
            alignment, index, hidden_states, index_to_aa = do_msa(group_seqs, group_names, group_seqnums, group_embeddings, logging, minscore1 = minscore1, alignment_group = i, args = args)
            alignments.append(alignment)
            index_to_aas_list.append(index_to_aa)
            hidden_states_list.append(hidden_states)
    
            cluster_order, clustid_to_clust = clusts_from_alignment(alignment)
    
            pring("attempt squish, merging adjacent non-overlapping clusters")       
            for rep in range(0,10):
                 prevclust = alignment
                 cluster_order, clustid_to_clust = squish_clusters(alignment, index, hidden_states, index_to_aa)                
                 alignment = make_alignment(cluster_order, group_seqnums, clustid_to_clust, group_names)
                 if alignment.alignment == prevclust:
                        break
            alignment = make_alignment(cluster_order, group_seqnums, clustid_to_clust, group_names)
             
            
            if pca_plot: 
                png_align_out = "{}/{}.alignment_group{}.fasta.png".format(record_dir, outfile_name,  i)
                do_pca_plot(hidden_states, index_to_aa, png_align_out, clustid_to_clust = clustid_to_clust)
     
          
            aln_fasta_list_group = []
    
            fasta_align_i = alignment.format_aln("fasta")
            clustal_align_i = alignment.format_aln("clustal")   #alignment_ic(alignment, group_names)
    
            aln_fasta_list.append([">" + x for x in fasta_align_i.split(">")][1:]) # fasta_align_i.split("\n")) 
    
    
            fasta_align_out = "{}/{}.alignment_group{}.fasta.aln".format(record_dir, outfile_name, i)
            with open(fasta_align_out, "w") as o:
                  o.write(fasta_align_i)
    
            clustal_align_out = "{}/{}.alignment_group{}.clustal.aln".format(record_dir, outfile_name, i)
            with open(clustal_align_out, "w") as o:
                  o.write(clustal_align_i)

            # If nothing to merge
            if len(cluster_names_list) == 1 and (( len(excluded_records) == 0 or fully_exclude == True )) :
                with open(outfile, "w") as o:
                      o.write(clustal_align_i)
                sys.exit()
           
         

            
    
   
    consolidator = "mafft"
    if consolidator == "mafft":
      if len(cluster_names_list) > 1 or ( len(excluded_records) > 0 or fully_exclude == False ) :
    
        seq_count = 1
    
        


        ic("aln_fasta_list", aln_fasta_list)
        with open("{}/{}.all_fastas_aln.fasta".format(record_dir, outfile_name,), "w") as o:
    
            with open("{}/{}.key_table.txt".format(record_dir, outfile_name), "w") as tb:
                for k in range(len(aln_fasta_list)):
                  
                   for s in range(len(aln_fasta_list[k])):
                        o.write("{}\n".format(aln_fasta_list[k][s]))
                        tb.write("{} ".format(seq_count))
                        seq_count = seq_count + 1
                   tb.write("\n")
        

        try:
            
            # Work on path to mafft
            #os.system("mafft --clustalout --merge key_table.txt --auto all_fastas_aln.fasta > {}".format(outfile))
            os.system("singularity exec /scratch/gpfs/cmcwhite/mafft_7.475.sif mafft --clustalout --merge {}/{}.key_table.txt --auto {}/{}.all_fastas_aln.fasta > {}".format(record_dir, outfile_name, record_dir, outfile_name, outfile))
               
            os.system("cat {}".format(outfile))
        except Exception as E:
            ic("Not doing mafft merge") 
    
    for i in range(len(alignments)):
                    mylist = []
                    cluster_order, clustid_to_clust = clusts_from_alignment(alignments[i])
                    for key, value in clustid_to_clust.items():
                          clustid_embeddings = []
                          indexes = [x.index for x in value]
                          clustid_embeddings = np.take(hidden_states_list[i], indexes, 0)
                          clustid_embeddings = normalize(clustid_embeddings, axis =1, norm = "l2")
                          if len(indexes) > 1:
                              cosim = cosine_similarity(clustid_embeddings)
                              upper = cosim[np.triu_indices(cosim.shape[0], k = 1)]
                              #ic(upper)
                              mean_cosim = np.mean(upper)
                          else:
                             mean_cosim = 0
                          ic(key, mean_cosim, len(indexes))






#@profile
def consolidate_w_clustering(clusters_dict, seqs_aas_dict):
    return(0) 
